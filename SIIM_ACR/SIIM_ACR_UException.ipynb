{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#!pip install -U efficientnet==0.0.4\nimport numpy as np\nimport pandas as pd\nimport gc\nimport keras\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport glob\nimport shutil\nimport os\nimport random\nfrom PIL import Image\n\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n    \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir masks\n!unzip -q ../input/data-repack-and-image-statistics/masks.zip -d masks \n!mkdir train\n!unzip -q ../input/data-repack-and-image-statistics/train.zip -d train \n!mkdir test\n!unzip -q ../input/data-repack-and-image-statistics/test.zip -d test ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_mask_fn = glob.glob('./masks/*')\nmask_df = pd.DataFrame()\nmask_df['file_names'] = all_mask_fn\nmask_df['mask_percentage'] = 0\nmask_df.set_index('file_names',inplace=True)\nfor fn in all_mask_fn:\n    mask_df.loc[fn,'mask_percentage'] = np.array(Image.open(fn)).sum()/(256*256*255) #255 is bcz img range is 255\n    \nmask_df.reset_index(inplace=True)\nsns.distplot(mask_df.mask_percentage)\nmask_df['labels'] = 0\nmask_df.loc[mask_df.mask_percentage>0,'labels'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train_fn = glob.glob('./train/*')\ntotal_samples = len(all_train_fn)\nidx = np.arange(total_samples)\ntrain_fn,val_fn = train_test_split(all_train_fn,stratify=mask_df.labels,test_size=0.1,random_state=10)\n\nprint('No. of train files:', len(train_fn))\nprint('No. of val files:', len(val_fn))\n\nmasks_train_fn = [fn.replace('./train','./masks') for fn in train_fn]    \nmasks_val_fn = [fn.replace('./train','./masks') for fn in val_fn]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!mkdir ./keras_im_train\ntrain_dir = './keras_im_train'\nfor full_fn in train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_train\ntrain_dir = './keras_mask_train'\nfor full_fn in masks_train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_im_val\ntrain_dir = './keras_im_val'\nfor full_fn in val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_val\ntrain_dir = './keras_mask_val'\nfor full_fn in masks_val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_im_path, train_mask_path = './keras_im_train','./keras_mask_train'\nh, w, batch_size = 256, 256, 16\n\nval_im_path, val_mask_path = './keras_im_val','./keras_mask_val'\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, train_im_path=train_im_path,train_mask_path=train_mask_path,\n                 augmentations=None, batch_size=batch_size, img_size=h, n_channels=3, shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_im_paths = glob.glob(train_im_path+'/*')\n        \n        self.train_im_path = train_im_path\n        self.train_mask_path = train_mask_path\n\n        self.img_size = img_size\n        \n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.augment = augmentations\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.train_im_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.train_im_paths))]\n\n        # Find list of IDs\n        list_IDs_im = [self.train_im_paths[k] for k in indexes]\n\n        # Generate data\n        X, y = self.data_generation(list_IDs_im)\n\n        if self.augment is None:\n            return X,np.array(y)/255\n        else:            \n            im,mask = [],[]   \n            for x,y in zip(X,y):\n                augmented = self.augment(image=x, mask=y)\n                im.append(augmented['image'])\n                mask.append(augmented['mask'])\n            return np.array(im),np.array(mask)/255\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_im_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, list_IDs_im):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_im),self.img_size,self.img_size, self.n_channels))\n        y = np.empty((len(list_IDs_im),self.img_size,self.img_size, 1))\n\n        # Generate data\n        for i, im_path in enumerate(list_IDs_im):\n            \n            im = np.array(Image.open(im_path))\n            mask_path = im_path.replace(self.train_im_path,self.train_mask_path)\n            \n            mask = np.array(Image.open(mask_path))\n            \n            \n            if len(im.shape)==2:\n                im = np.repeat(im[...,None],3,2)\n\n#             # Resize sample\n            X[i,] = cv2.resize(im,(self.img_size,self.img_size))\n\n            # Store class\n            y[i,] = cv2.resize(mask,(self.img_size,self.img_size))[..., np.newaxis]\n            y[y>0] = 255\n\n        return np.uint8(X),np.uint8(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install albumentations > /dev/null \nimport cv2\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    OneOf([IAAAdditiveGaussianNoise(), GaussNoise(), ], p=0.2),\n    OneOf([MotionBlur(p=0.2), MedianBlur(blur_limit=3, p=0.1), Blur(blur_limit=3, p=0.1), ], p=0.2),\n    OneOf([RandomContrast(), RandomGamma(), RandomBrightness(), ], p=0.25),\n    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=5, p=0.2),\n    OneOf([ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03), GridDistortion(), OpticalDistortion(distort_limit=2, shift_limit=0.5), ], p=0.3),\n    RandomSizedCrop(min_max_height=(156, 256), height=h, width=w, p=0.25),\n    ToFloat(max_value=1)\n],p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    ToFloat(max_value=1)\n],p=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45410fdbd533d546a578d9dc29982a1657b8dfa9","trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\ndef get_iou_vector(A, B):\n    # Numpy version    \n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d88d952ca7c27fd0e2411eb63ec16fd25cb8ebec","trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef focal_loss(y_true, y_pred):\n    gamma=0.75\n    alpha=0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n    pt_1 = K.clip(pt_1, 1e-3, .999)\n    pt_0 = K.clip(pt_0, 1e-3, .999)\n\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n\ndef criterion_pixel(truth_pixel, logit_pixel, weighted=False):\n    logit = K.flatten(logit_pixel)\n    truth = K.flatten(truth_pixel)\n\n    loss = K.binary_crossentropy(truth, logit, from_logits=True)\n\n    if weighted:\n        pos = K.cast(K.greater(truth, 0.5), dtype='float32')\n        neg = K.cast(K.less(truth, 0.5), dtype='float32')\n        pos_weight = K.sum(pos) + K.epsilon()\n        neg_weight = K.sum(neg) + K.epsilon()\n        loss = K.sum(0.25*pos*loss/pos_weight + 0.75*neg*loss/neg_weight)\n    else:\n        loss = K.mean(loss)\n\n    return loss\n\ndef criterion_pixel(truth_pixel, logit_pixel):\n    batch_size = K.shape(logit_pixel)[0]\n    logit = K.reshape(logit_pixel, (batch_size,-1))\n    truth = K.reshape(truth_pixel, (batch_size,-1))\n\n    loss = soft_dice_criterion(truth, logit)\n\n    loss = K.mean(loss)\n    return loss\n\ndef soft_dice_criterion(truth, logit, weight=[0.2,0.8]):\n\n    batch_size = K.shape(logit)[0]\n    probability = K.sigmoid(logit)\n\n    p = K.reshape(probability, (batch_size,-1))\n    t = K.reshape(truth, (batch_size,-1))\n    w = tf.where(K.equal(t, 1), weight[1] * K.ones_like(t), weight[0] * K.ones_like(t))\n\n    p = w*(p*2-1)  #convert to [0,1] --> [-1, 1]\n    t = w*(t*2-1)\n\n    intersection = K.sum(p * t)\n    union =  K.sum(p * p) + K.sum(t * t)\n    dice  = 1 - 2*intersection/union\n\n    loss = dice\n    return loss\n\ndef lovasz_loss(truth, logit, margin=[1,5]):\n\n    def compute_lovasz_gradient(truth): #sorted\n        truth_sum    = K.sum(truth)\n        intersection = truth_sum - K.cumsum(truth, 0)\n        union        = truth_sum + K.cumsum(1 - truth, 0)\n        jaccard      = 1. - intersection / union\n        jaccard      = K.concatenate([jaccard[0:1], jaccard[1:] - jaccard[:-1]], axis=0)\n\n        gradient     = jaccard\n        return gradient\n\n    def lovasz_hinge_one(truth , logit):\n\n        m = tf.where(K.equal(truth, 1), margin[1] * K.ones_like(truth), margin[0] * K.ones_like(truth))\n\n        truth = K.cast(truth, dtype = logit.dtype)\n        sign  = 2. * truth - 1.\n        hinge = (m - logit * K.stop_gradient(sign))\n        hinge, permutation = tf.nn.top_k(hinge, k=K.shape(hinge)[0])\n        hinge = K.relu(hinge)\n\n        truth = K.gather(truth, permutation)\n        gradient = compute_lovasz_gradient(truth)\n\n        loss = K.dot(K.expand_dims(hinge, 0), K.stop_gradient(K.expand_dims(gradient, -1)))\n\n        return loss\n\n    #----\n    batch_size = K.shape(logit)[0]\n    loss = K.map_fn(lambda x: lovasz_hinge_one(truth[x], logit[x]), K.arange(batch_size), dtype='float32')\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98c8b0aff0989293ce43592a82eb3c5dcf8bbee9","trusted":true},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_my_iou_metric', mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            #callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)          \n            callbacks.ReduceLROnPlateau(monitor='val_my_iou_metric', mode = 'max', factor=0.25, patience=5, min_lr=0.0001, verbose=1)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8e87341f77e48a6c54de11f8dc72f50f43b8637","trusted":true},"cell_type":"code","source":"def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    x = BatchNormalization()(x)\n    if activation == True:\n        x = LeakyReLU(alpha=0.1)(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16):\n    x = LeakyReLU(alpha=0.1)(blockInput)\n    x = BatchNormalization()(x)\n    blockInput = BatchNormalization()(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.xception import Xception\n\ndef UXception(input_shape=(None, None, 3)):\n\n    backbone = Xception(input_shape=input_shape,weights='imagenet',include_top=False)\n    input = backbone.input\n    start_neurons = 16\n\n    conv4 = backbone.layers[121].output\n    conv4 = LeakyReLU(alpha=0.1)(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    #pool4 = Dropout(0.1)(pool4)\n    \n     # Middle\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = LeakyReLU(alpha=0.1)(convm)\n    \n    # 10 -> 20\n    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.1)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n    \n    # 10 -> 20\n    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    conv3 = backbone.layers[31].output\n    uconv3 = concatenate([deconv3, conv3])    \n    #uconv3 = Dropout(0.1)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n\n    # 20 -> 40\n    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    conv2 = backbone.layers[21].output\n    conv2 = ZeroPadding2D(((1,0),(1,0)))(conv2)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(0.1)(uconv2)\n    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n    \n    # 40 -> 80\n    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    conv1 = backbone.layers[11].output\n    conv1 = ZeroPadding2D(((3,0),(3,0)))(conv1)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    #uconv1 = Dropout(0.1)(uconv1)\n    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n    \n    # 80 -> 160\n    deconv0 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)\n    \n    uconv0 = Dropout(0.1)(deconv0)\n    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = LeakyReLU(alpha=0.1)(uconv0) \n\n    # 160 -> 256\n    uconv = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(1, 1), padding=\"same\")(uconv0)   \n    #uconv = Dropout(0.1)(uconv)\n    uconv = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv)\n    uconv = residual_block(uconv,start_neurons * 1)\n    uconv = residual_block(uconv,start_neurons * 1)\n      \n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(uconv)\n    output_layer = Activation('sigmoid')(output_layer_noActi)\n    \n    model = Model(input, output_layer)\n    model.name = 'u-xception'\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nmodel = UXception(input_shape=(h, w, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model0.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d7a02b2dca5074cbd7025947d031b134a17ff54","trusted":true},"cell_type":"code","source":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"trusted":true},"cell_type":"code","source":"epochs = 70\nswa_epoch = max(1, epochs-3)\nimg_size = 256\nbatch_size = 16\n\n# Generators\nvalid_im_path,valid_mask_path = './keras_im_val','./keras_mask_val'\n\ntraining_generator = DataGenerator(augmentations=AUGMENTATIONS_TRAIN, img_size=img_size)\nvalidation_generator = DataGenerator(train_im_path=valid_im_path, train_mask_path=valid_mask_path, augmentations=AUGMENTATIONS_TEST, img_size=img_size)\n\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nswa = SWA('./keras_swa.model',swa_epoch)\n\nmodel.compile(loss=bce_dice_loss, optimizer='adam', metrics=[my_iou_metric])\nhistory = model.fit_generator(generator=training_generator, validation_data=validation_generator, use_multiprocessing=False, \n                              epochs=epochs,verbose=True, callbacks=snapshot.get_callbacks())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4)) \nplt.subplot(1,2,1)\nplt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.ylabel('iou')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\n\nplt.title('model IOU')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'][1:])\nplt.plot(history.history['val_loss'][1:])\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.title('model loss')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","trusted":true},"cell_type":"code","source":"# Load best model or swa model if not available\ntry:\n    print('using swa weight model')\n    model.load_weights('./keras_swa.model')\nexcept Exception as e:\n    print(e)\n    model.load_weights('./keras.model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc4d63ca6c7e6c13e4cfb988a554199712486af2","trusted":true},"cell_type":"code","source":"def predict_result(model,validation_generator,img_size): \n    # TBD predict both orginal and reflect x\n    preds_test1 = model.predict_generator(validation_generator).reshape(-1, img_size, img_size)\n    return preds_test1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","trusted":true},"cell_type":"code","source":"validation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,shuffle=False)\n\nAUGMENTATIONS_TEST_FLIPPED = Compose([\n    HorizontalFlip(),\n    ToFloat(max_value=1)\n],p=1)\n\nvalidation_generator_flipped = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST_FLIPPED,\n                                     img_size=img_size,shuffle=False)\n\npreds_valid_orig = predict_result(model,validation_generator,img_size)\npreds_valid_flipped = predict_result(model,validation_generator_flipped,img_size)\npreds_valid_flipped = np.array([np.fliplr(x) for x in preds_valid_flipped])\npreds_valid = 0.5*preds_valid_orig + 0.5*preds_valid_flipped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","trusted":true},"cell_type":"code","source":"threshold_best = 0.5\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)\n\nvalid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scoring for last model\nthresholds = np.linspace(0.2, 0.9, 31)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0123d4cbd90c49c822cf5dc545a30f4a9eb456"},"cell_type":"markdown","source":"# Test Set Prediction"},{"metadata":{"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42","trusted":true},"cell_type":"code","source":"test_fn = glob.glob('./test/*')\nx_test = [cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in test_fn]\nx_test = np.array(x_test)\nx_test = np.array([np.repeat(im[...,None],3,2) for im in x_test])\nprint(x_test.shape)\npreds_test_orig = model.predict(x_test,batch_size=batch_size)\n\nx_test = np.array([np.fliplr(x) for x in x_test])\npreds_test_flipped = model.predict(x_test,batch_size=batch_size)\npreds_test_flipped = np.array([np.fliplr(x) for x in preds_test_flipped])\n\npreds_test = 0.5*preds_test_orig + 0.5*preds_test_flipped\n\n# del x_test; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b63b8f23f51fb7030cbe6f10d38b186044dc7d4e","trusted":true},"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n# for i, idx in enumerate(index_val[:max_images]):\nfor i, idx in enumerate(test_fn[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i].squeeze()\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best).T, dtype=np.float32), alpha=0.5, cmap=\"Reds\")\n    ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"388d2d738bd15cc4b7259d1ec41df6e4eede94e7","trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nfrom mask_functions import rle2mask,mask2rle\nimport pdb\n\n# Generate rle encodings (images are first converted to the original size)\nrles = []\ni,max_img = 1,10\nplt.figure(figsize=(16,4))\nfor p in tqdm_notebook(preds_test):\n    p = p.squeeze()\n    im = cv2.resize(p,(1024,1024))\n    im = im > threshold_best\n#     zero out the smaller regions.\n    if im.sum()<1024*2:\n        im[:] = 0\n    im = (im.T*255).astype(np.uint8)  \n    rles.append(mask2rle(im, 1024, 1024))\n    i += 1\n    if i<max_img:\n        plt.subplot(1,max_img,i)\n        plt.imshow(im)\n        plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d85641bbb14e796c7f47a6122f2b9ed2c97a46f","trusted":true},"cell_type":"code","source":"ids = [o.split('/')[-1][:-4] for o in test_fn]\nsub_df = pd.DataFrame({'ImageId': ids, 'EncodedPixels': rles})\nsub_df.loc[sub_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -r */","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Leak Correction\n\nhttps://www.kaggle.com/raddar/sample-submission-leak"},{"metadata":{"trusted":true},"cell_type":"code","source":"leak_sub = pd.read_csv('../input/sample-submission-leak/leaky_unet_submission.csv',index_col='ImageId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"leak_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.set_index('ImageId',inplace=True)\nidx = leak_sub[leak_sub.EncodedPixels=='-1'].index\nsub_df.loc[idx] = '-1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('leak_correction.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"25c2207c56db4b488df8f11f902f214d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3eff6e77140943149ae6f17c8ae0656f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65be1342ca71431080466fc07efd608f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7d7ee049921444087ce3b73e3ebaf97","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d75108033fd749e4a6326defb1de32d6","value":31}},"674835ee0bad43c8a09559a1966a4d2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3137845f22431d96389a29a49620f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bf014f05120436ba535efa18c10d07d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}},"940ded6bb9c547969e649db541adb119":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_674835ee0bad43c8a09559a1966a4d2a","placeholder":"​","style":"IPY_MODEL_f428f65de51a4dfeaa7c2609595cf8fe","value":"100% 1377/1377 [11:41&lt;00:00,  1.97it/s]"}},"9db07e0b377b41ccab01f9c82b9d5bb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65be1342ca71431080466fc07efd608f","IPY_MODEL_ca4e3167d0ab4cd4a409d4c17f4e1e5f"],"layout":"IPY_MODEL_6d3137845f22431d96389a29a49620f6"}},"b1d4361772ac4d38b33c1fd8e8e4a258":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba15fdf293c645cdb9024b9cbae1ebeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4c8fb0c5a144825aba941ca30c7c923":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0f9d1c1ee3f4ccfac924a680627450f","IPY_MODEL_940ded6bb9c547969e649db541adb119"],"layout":"IPY_MODEL_ba15fdf293c645cdb9024b9cbae1ebeb"}},"ca4e3167d0ab4cd4a409d4c17f4e1e5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1d4361772ac4d38b33c1fd8e8e4a258","placeholder":"​","style":"IPY_MODEL_8bf014f05120436ba535efa18c10d07d","value":"100% 31/31 [02:57&lt;00:00,  5.71s/it]"}},"d0f9d1c1ee3f4ccfac924a680627450f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.4.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_25c2207c56db4b488df8f11f902f214d","max":1377,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3eff6e77140943149ae6f17c8ae0656f","value":1377}},"d75108033fd749e4a6326defb1de32d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7d7ee049921444087ce3b73e3ebaf97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.1.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.1.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f428f65de51a4dfeaa7c2609595cf8fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.4.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.4.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.1.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}