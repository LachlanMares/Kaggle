{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'train.csv', 'test.csv', 'embeddings']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in  \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Add, Dropout, SpatialDropout1D, Conv1D, BatchNormalization, Activation\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Reshape, Flatten, Conv2D, MaxPool2D, Permute, Multiply\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers \n",
    "from keras.initializers import *\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, one_hot \n",
    "from keras.utils import to_categorical \n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau \n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import gc\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1fe136edb1723e54beebe596c9926a70139b7212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QIQC_v12\n"
     ]
    }
   ],
   "source": [
    "version = 12\n",
    "basic_name = f'QIQC_v{version}'\n",
    "save_model_name_0 = basic_name + '_0.model'\n",
    "\n",
    "print(basic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    \n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "sub = test_df[['qid']]\n",
    "\n",
    "question_texts_df = pd.merge(train_df, test_df, on='question_text', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e3015983b62dbc9f2670f5a8bd1cf58af20e11a0"
   },
   "outputs": [],
   "source": [
    "X_train = train_df[\"question_text\"].fillna(\" \").values\n",
    "X_test = test_df[\"question_text\"].fillna(\" \").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3cd000cec0f06b015af485e6d2d33ebe8419f6e4"
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "num_words = 72\n",
    "max_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "51a5a6e75c5e84c86af51c62c2cd70f9a6f2d900"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(question_texts_df['question_text']))\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ad1b59c46a8683048531ed8e10ee0edc64a48a73"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=num_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=num_words)\n",
    "\n",
    "y_train = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "3a3b07a37cdf453af617e604fd1658a97e107ed5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df, test_df, question_texts_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ddde52fac8e5a5473a5d8dc719dc870ff2b46949"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "0a1eff07150af47cf326e28c99ee12ce8b885cfd"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "ed998d345621ef5fe34378c3fa98b5118de6975a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(196177, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.mean((load_glove(word_index), load_para(word_index)), axis=0)  \n",
    "\n",
    "gc.collect()\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "35e69cc0c823fca4d83c0d6e7460b2b7c00172a1"
   },
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    TIME_STEPS = inputs.shape[1].value\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    \n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "ff45c0ecafb52a06f69afbe2669f9b47f7f33c61"
   },
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c7318bce463a4df4e5b3cf551bf29e51cfbd9ff0"
   },
   "outputs": [],
   "source": [
    "def combined_global_pool(blockInput):\n",
    "    max_pool = GlobalMaxPooling1D()(blockInput)\n",
    "    avg_pool = GlobalAveragePooling1D()(blockInput)\n",
    "    return concatenate([avg_pool, max_pool])\n",
    "\n",
    "def combined_pool(blockInput):\n",
    "    max_pool = MaxPooling1D(pool_size=3, strides=None)(blockInput)\n",
    "    avg_pool = AveragePooling1D(pool_size=3, strides=None)(blockInput)\n",
    "    return concatenate([avg_pool, max_pool])\n",
    "\n",
    "def build_model_gru_capsule(num_words, max_features, embed_size, embedding_matrix, gru_units, dense_units, dropout):\n",
    "    K.clear_session()\n",
    "    imp_layer = Input(shape=(num_words, ))\n",
    "      \n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(imp_layer)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(gru_units, return_sequences=True, kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "    xa = attention_3d_block(x)\n",
    "    xa = combined_global_pool(xa)\n",
    "    xa = BatchNormalization()(xa)    \n",
    "    xa = Dense(dense_units*2, activation=\"relu\")(xa) \n",
    "    \n",
    "    xc = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "    xc = Flatten()(xc)\n",
    "    xc = BatchNormalization()(xc)\n",
    "    xc = Dense(dense_units*2, activation='relu')(xc)\n",
    "    \n",
    "    x = concatenate([xa, xc])\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_units, activation='relu')(x)\n",
    "       \n",
    "    out_layer = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=imp_layer, outputs=out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "fa1095bb16a4942162ef224872e0d4cc097ddd82"
   },
   "outputs": [],
   "source": [
    "model0 = build_model_gru_capsule(num_words=num_words, max_features=max_features, embed_size=embedding_matrix.shape[1], embedding_matrix=embedding_matrix, gru_units=64, dense_units=32, dropout=0.20)\n",
    "model0.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "bd015cbe901553dfdab9b76ea788584d75bb8347"
   },
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "13dcb855c887e70cfaa8b2b968a72c8cbcece816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 72)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 72, 300)      58853100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 72, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 72, 128)      140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 128, 72)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 128, 72)      0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128, 72)      5256        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 72, 128)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 72, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "capsule_1 (Capsule)             (None, 10, 10)       12800       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 100)          0           capsule_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           6464        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128)          0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           4128        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            33          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 59,040,709\n",
      "Trainable params: 186,641\n",
      "Non-trainable params: 58,854,068\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/6\n",
      " - 219s - loss: 0.1281 - acc: 0.9501 - val_loss: 0.1074 - val_acc: 0.9568\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.10736, saving model to QIQC_v12_0.model\n",
      "Epoch 2/6\n",
      " - 214s - loss: 0.1093 - acc: 0.9566 - val_loss: 0.1025 - val_acc: 0.9588\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.10736 to 0.10249, saving model to QIQC_v12_0.model\n",
      "Epoch 3/6\n",
      " - 214s - loss: 0.1041 - acc: 0.9585 - val_loss: 0.0997 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10249 to 0.09966, saving model to QIQC_v12_0.model\n",
      "Epoch 4/6\n",
      " - 214s - loss: 0.1006 - acc: 0.9597 - val_loss: 0.0991 - val_acc: 0.9604\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09966 to 0.09906, saving model to QIQC_v12_0.model\n",
      "Epoch 5/6\n",
      " - 214s - loss: 0.0976 - acc: 0.9606 - val_loss: 0.0981 - val_acc: 0.9605\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09906 to 0.09805, saving model to QIQC_v12_0.model\n",
      "Epoch 6/6\n",
      " - 215s - loss: 0.0951 - acc: 0.9617 - val_loss: 0.0978 - val_acc: 0.9609\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09805 to 0.09776, saving model to QIQC_v12_0.model\n",
      "Optimal F1: 0.6876 at threshold: 0.2955\n",
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/6\n",
      " - 214s - loss: 0.0952 - acc: 0.9615 - val_loss: 0.0915 - val_acc: 0.9628\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09149, saving model to QIQC_v12_0.model\n",
      "Epoch 2/6\n",
      " - 215s - loss: 0.0930 - acc: 0.9623 - val_loss: 0.0897 - val_acc: 0.9639\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09149 to 0.08970, saving model to QIQC_v12_0.model\n",
      "Epoch 3/6\n",
      " - 215s - loss: 0.0910 - acc: 0.9630 - val_loss: 0.0914 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.08970\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 4/6\n",
      " - 215s - loss: 0.0875 - acc: 0.9641 - val_loss: 0.0905 - val_acc: 0.9633\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.08970\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
      "Epoch 5/6\n",
      " - 214s - loss: 0.0844 - acc: 0.9652 - val_loss: 0.0902 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.08970\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
      "Epoch 6/6\n",
      " - 215s - loss: 0.0827 - acc: 0.9661 - val_loss: 0.0903 - val_acc: 0.9640\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.08970\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
      "Optimal F1: 0.7135 at threshold: 0.3221\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 215s - loss: 0.0895 - acc: 0.9635 - val_loss: 0.0794 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07944, saving model to QIQC_v12_0.model\n",
      "Epoch 2/6\n",
      " - 215s - loss: 0.0885 - acc: 0.9640 - val_loss: 0.0792 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.07944 to 0.07925, saving model to QIQC_v12_0.model\n",
      "Epoch 3/6\n",
      " - 215s - loss: 0.0874 - acc: 0.9642 - val_loss: 0.0792 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07925 to 0.07920, saving model to QIQC_v12_0.model\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 4/6\n",
      " - 215s - loss: 0.0865 - acc: 0.9647 - val_loss: 0.0791 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07920 to 0.07911, saving model to QIQC_v12_0.model\n",
      "Epoch 5/6\n",
      " - 215s - loss: 0.0861 - acc: 0.9648 - val_loss: 0.0794 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07911\n",
      "Epoch 6/6\n",
      " - 215s - loss: 0.0856 - acc: 0.9650 - val_loss: 0.0793 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07911\n",
      "Optimal F1: 0.7449 at threshold: 0.3634\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 215s - loss: 0.0868 - acc: 0.9647 - val_loss: 0.0766 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07659, saving model to QIQC_v12_0.model\n",
      "Epoch 2/6\n",
      " - 215s - loss: 0.0862 - acc: 0.9649 - val_loss: 0.0768 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07659\n",
      "Epoch 3/6\n",
      " - 215s - loss: 0.0855 - acc: 0.9651 - val_loss: 0.0769 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07659\n",
      "Epoch 4/6\n",
      " - 215s - loss: 0.0851 - acc: 0.9651 - val_loss: 0.0772 - val_acc: 0.9677\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07659\n",
      "Epoch 5/6\n",
      " - 215s - loss: 0.0848 - acc: 0.9656 - val_loss: 0.0774 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07659\n",
      "Epoch 6/6\n",
      " - 215s - loss: 0.0842 - acc: 0.9656 - val_loss: 0.0776 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07659\n",
      "Optimal F1: 0.7522 at threshold: 0.3641\n",
      "Train on 1044898 samples, validate on 261224 samples\n",
      "Epoch 1/6\n",
      " - 215s - loss: 0.0863 - acc: 0.9647 - val_loss: 0.0761 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07607, saving model to QIQC_v12_0.model\n",
      "Epoch 2/6\n",
      " - 215s - loss: 0.0858 - acc: 0.9650 - val_loss: 0.0763 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07607\n",
      "Epoch 3/6\n",
      " - 215s - loss: 0.0854 - acc: 0.9651 - val_loss: 0.0765 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07607\n",
      "Epoch 4/6\n",
      " - 215s - loss: 0.0850 - acc: 0.9654 - val_loss: 0.0767 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07607\n",
      "Epoch 5/6\n",
      " - 215s - loss: 0.0844 - acc: 0.9654 - val_loss: 0.0768 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07607\n",
      "Epoch 6/6\n",
      " - 215s - loss: 0.0840 - acc: 0.9656 - val_loss: 0.0772 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07607\n",
      "Optimal F1: 0.7541 at threshold: 0.3809\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "bestscore = []\n",
    "y_test = np.zeros((X_test.shape[0], ))\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    X__train, X_val, Y__train, Y_val = X_train[train_index], X_train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(save_model_name_0, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    \n",
    "    if i == 0:print(model0.summary())\n",
    "    model0.fit(X__train, Y__train, batch_size=512, epochs=6, validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks,)\n",
    "    model0.load_weights(save_model_name_0)\n",
    "    \n",
    "    y_pred = model0.predict([X_val], batch_size=1024, verbose=2)\n",
    "    y_test += np.squeeze(model0.predict([X_test], batch_size=1024, verbose=2))/5\n",
    "    \n",
    "    f1, threshold = f1_smart(np.squeeze(Y_val), np.squeeze(y_pred))\n",
    "    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))\n",
    "    bestscore.append(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "9ba667091418d9aea4184d4e882142b6b3b3e55a"
   },
   "outputs": [],
   "source": [
    "y_test = y_test.reshape((-1, 1))\n",
    "pred_test_y = (y_test>np.mean(bestscore)).astype(int)\n",
    "sub['prediction'] = pred_test_y\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
